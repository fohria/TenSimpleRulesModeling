{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Box 6 - Improving parameter recovery by modeling unimportant parameters\n",
    "\n",
    "TODO: move simulation, likelihood and fitting functions to subfolders when we have decided it's me or the paper being confused.\n",
    "\n",
    "This box confuses me. The paper makes it sound like we can modify, for example, Model 3 by adding a bias and thereby capturing some randomness/noise in the simulated data so that the two main parameters - alpha and beta - are recovered better. The title of Box 6 even explicitly says \"Example: improving parameter recovery by modeling unimportant parameters.\"\n",
    "\n",
    "But what they do is simulate a biased model - let's call it Model B - and show that a fitting Model B provides better fit than the model without bias, Model M3.\n",
    "\n",
    "From the paper:\n",
    "\n",
    "> \"We then simulated behavior with this model for a range of parameter values and fit the model with the original version of model 3, without the bias, and the modified version of model 3, with the bias.\"\n",
    "\n",
    "...\n",
    "\n",
    "Yes? You simulated with bias so what are we supposed to learn here? That you're simulating a hypothetical person with such a bias? If so, the bias isn't an \"unimportant\" parameter, it's actually \"there\" so the biased model is actually a better model, not \"unimportant\", right?\n",
    "\n",
    "I probably misunderstand again, but wouldn't a more interesting test be to simulate with Model M3 and see if Model B helps stabilize recovery of alpha & beta parameters?\n",
    "\n",
    "So that's what I've done below..\n",
    "\n",
    "## function and library imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from numba import njit, int32\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "from SimulationFunctions.simulate_M3RescorlaWagner import simulate_M3RescorlaWagner\n",
    "from LikelihoodFunctions.lik_M3RescorlaWagner import lik_M3RescorlaWagner\n",
    "\n",
    "# reset seaborn to get decent looking grid\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Task description and simulation\n",
    "\n",
    "The task used in this box is a variant of the standard bandit task used previously. But here we have 10 blocks, each with 50 trials. Every other block we switch which \"arm\" of the bandit is the better one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "@njit\n",
    "def simulate():\n",
    "\n",
    "    # using similar parameter value generation as matlab code\n",
    "    alpha = 0.1 + 0.4 * np.random.rand()\n",
    "    beta = 1 + 8 * np.random.rand()\n",
    "    trialcount = 50\n",
    "    block_count = 10\n",
    "\n",
    "    actions = np.zeros(trialcount * block_count, dtype=int32)\n",
    "    rewards = np.zeros_like(actions, dtype=int32)\n",
    "\n",
    "    for block in range(block_count):\n",
    "        if block % 2 == 0:\n",
    "            bandit = np.array([0.2, 0.8])\n",
    "        else:\n",
    "            bandit = np.array([0.8, 0.2])\n",
    "\n",
    "        a_block, r_block = simulate_M3RescorlaWagner(\n",
    "            trialcount, bandit, alpha, beta)\n",
    "\n",
    "        actions[block * 50:(block + 1) * 50] = a_block\n",
    "        rewards[block * 50:(block + 1) * 50] = r_block\n",
    "\n",
    "    return actions, rewards, [alpha, beta]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Model M3 likelihood adapted to the task\n",
    "\n",
    "We have to add the task procedures to our existing likelihood function for Model M3 like we did for the simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "@njit\n",
    "def llh_m3(parameters, actions, rewards):\n",
    "\n",
    "    block_count = 10\n",
    "    total_llh = 0\n",
    "\n",
    "    for block in range(block_count):\n",
    "        a_block = actions[block * 50:(block + 1) * 50]\n",
    "        r_block = rewards[block * 50:(block + 1) * 50]\n",
    "        total_llh += lik_M3RescorlaWagner(\n",
    "            parameters, a_block, r_block\n",
    "        )\n",
    "\n",
    "    return total_llh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Model M likelihood\n",
    "\n",
    "We also need a new likelihood function for Model M\n",
    "\n",
    "According to the paper, we calculate the probability of choosing \"left\" according to:\n",
    "\n",
    "$$p^{left}_t = \\frac{1}{1 + exp( \\beta (Q^{right}_t−Q^{left}_t − B))}$$\n",
    "\n",
    "where $\\beta$ is similar to the inverse temperature of softmax and $B$ is the bias. Paper says left but the code below instead follow the matlab code where it's $p^{right}$ or rather `p2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "@njit\n",
    "def llh_m3bias(parameters, actions, rewards):\n",
    "\n",
    "    alpha = parameters[0]\n",
    "    beta = parameters[1]\n",
    "    bias = parameters[2]\n",
    "\n",
    "    Q = np.array([0.5, 0.5])\n",
    "\n",
    "    trialcount = len(actions)\n",
    "    choice_probs = np.zeros(trialcount)\n",
    "\n",
    "    for trial in range(trialcount):\n",
    "\n",
    "        if trial % 50 == 0:\n",
    "            Q = np.array([0.5, 0.5])\n",
    "\n",
    "        # paper mentions p^left but using matlab code variation\n",
    "        p2 = 1/(1 + np.exp(beta * (bias + Q[0] - Q[1])))\n",
    "        probs = np.array([1 - p2, p2])\n",
    "\n",
    "        choice_probs[trial] = probs[actions[trial]]\n",
    "\n",
    "        delta = rewards[trial] - Q[actions[trial]]\n",
    "        Q[actions[trial]] += alpha * delta\n",
    "\n",
    "    loglikelihood = np.sum(np.log(choice_probs))\n",
    "    return -loglikelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## fitting wrapper functions\n",
    "\n",
    "Finally we create simple wrappers around the `scipy.optimize.minimize` function as we did in Box 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitm3(actions, rewards):\n",
    "\n",
    "    best_fit = 9999\n",
    "    best_params = []\n",
    "    for startpoint in range(10):\n",
    "        guess = np.random.rand(2)\n",
    "        bounds = np.array([(0.01, 0.99), (0.1, 10)])  # alpha, beta\n",
    "        result = minimize(\n",
    "            llh_m3, guess, args=(actions, rewards), bounds=bounds)\n",
    "\n",
    "        if result.fun < best_fit and result.success is True:\n",
    "            best_fit = result.fun\n",
    "            best_params = result.x\n",
    "\n",
    "    return best_fit, best_params\n",
    "\n",
    "\n",
    "def fitm3bias(actions, rewards):\n",
    "\n",
    "    best_fit = 9999\n",
    "    best_params = []\n",
    "    for startpoint in range(10):\n",
    "        guess = np.random.rand(3)\n",
    "        bounds = [\n",
    "            (0.01, 0.99),  # alpha\n",
    "            (0.1, 10),     # beta\n",
    "            (0.01, 0.99)   # bias\n",
    "        ]\n",
    "        result = minimize(\n",
    "            llh_m3bias, guess, args=(actions, rewards), bounds=bounds)\n",
    "\n",
    "        if result.fun < best_fit and result.success is True:\n",
    "            best_fit = result.fun\n",
    "            best_params = result.x\n",
    "\n",
    "    return best_fit, best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## experimental parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simfitcount = 100  # number of simulations and fitting runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## simulation and fitting runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data container\n",
    "data = []\n",
    "# data_m3 = []\n",
    "# data_m3b = []\n",
    "# real_params = []\n",
    "\n",
    "for simfit in range(simfitcount):\n",
    "\n",
    "    print(f\"simfitrun {simfit}\")\n",
    "\n",
    "    actions, rewards, parameters = simulate()\n",
    "\n",
    "    _, m3_params = fitm3(actions, rewards)\n",
    "    _, m3b_params = fitm3bias(actions, rewards)\n",
    "    #\n",
    "    # data_m3.append(\n",
    "    #     fitm3(actions, rewards))\n",
    "    # data_m3b.append(\n",
    "    #     fitm3bias(actions, rewards))\n",
    "    # real_params.append(parameters)\n",
    "\n",
    "    data.append((\n",
    "        parameters[0],  # real alpha\n",
    "        parameters[1],  # real beta\n",
    "        m3_params[0],   # model 3 alpha\n",
    "        m3_params[1],   # model 3 beta\n",
    "        m3b_params[0],  # model bias alpha\n",
    "        m3b_params[1],  # model bias beta\n",
    "        m3b_params[2]   # model bias bias\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ## Plot fitted and real values for Model 3\n",
    "\n",
    "# ### Real alpha vs fitted alpha for M3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\n",
    "    'real alpha',\n",
    "    'real beta',\n",
    "    'm3 alpha',\n",
    "    'm3 beta',\n",
    "    'mb alpha',\n",
    "    'mb beta',\n",
    "    'mb bias'\n",
    "]\n",
    "fit_results = pd.DataFrame(columns = columns, data = data)\n",
    "\n",
    "fig = sns.regplot(data = fit_results, x = 'real alpha', y = 'm3 alpha')\n",
    "fig = sns.lineplot(\n",
    "    x = np.linspace(0, 1, len(fit_results)),\n",
    "    y = np.linspace(0, 1, len(fit_results)),\n",
    "    style = True,\n",
    "    dashes = [(2, 2)],\n",
    "    legend = False\n",
    ")\n",
    "fig.set(xlim = (0, 0.6), ylim = (0, 1), title = \"M3 alpha\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Real beta vs fitted beta for M3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = sns.regplot(data = fit_results, x = 'real beta', y = 'm3 beta')\n",
    "fig = sns.lineplot(\n",
    "    x = np.linspace(0, 10, len(fit_results)),\n",
    "    y = np.linspace(0, 10, len(fit_results)),\n",
    "    style = True,\n",
    "    dashes = [(2, 2)],\n",
    "    legend = False\n",
    ")\n",
    "fig.set(xlim = (0, 10), ylim = (0, 10), title = \"M3 beta\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Plot fitted and real values for Model Bias\n",
    "\n",
    "### Real alpha vs fitted alpha for MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = sns.regplot(data = fit_results, x = 'real alpha', y = 'mb alpha')\n",
    "fig = sns.lineplot(\n",
    "    x = np.linspace(0, 1, len(fit_results)),\n",
    "    y = np.linspace(0, 1, len(fit_results)),\n",
    "    style = True,\n",
    "    dashes = [(2, 2)],\n",
    "    legend = False\n",
    ")\n",
    "fig.set(xlim = (0, 0.6), ylim = (0, 1), title = \"MB alpha\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Real beta vs fitted beta for MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = sns.regplot(data = fit_results, x = 'real beta', y = 'mb beta')\n",
    "fig = sns.lineplot(\n",
    "    x = np.linspace(0, 10, len(fit_results)),\n",
    "    y = np.linspace(0, 10, len(fit_results)),\n",
    "    style = True,\n",
    "    dashes = [(2, 2)],\n",
    "    legend = False\n",
    ")\n",
    "fig.set(xlim = (0, 10), ylim = (0, 10), title = \"MB beta\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Plots look basically the same for both models, that's not an error. What happens here is that the bias parameter becomes basically zero for all the simulations because, well, we simulated without a bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sns.histplot(fit_results['mb bias'], stat = 'probability')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Discussion\n",
    "\n",
    "I guess we *could* see using the biased model to simulate instead of what we did here as more \"interesting\" in that it shows that fitting the biased model can account for a participant being biased? I don't know, it feels like I'm missing something here."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
