{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Box 7 - Model validation where the fit model performs too well\n",
    "\n",
    "TODO: put functions in subfolders\n",
    "\n",
    "TODO: save to dataframe while simfitting, can then plot immediately without creating dataframes individually\n",
    "\n",
    "This one is a bit tricky. We have two models: blind and state-based (which I will sometimes call \"sighted\"). So far so good. What can be confusing (at least it was to me) is the difference in what is being plotted in Figure A, B and C. So based on the plots from the paper, here's what happens:\n",
    "\n",
    "**Figure A:** We simulate the behaviour of both models and plot their performance. The two lines represent two different models, each with its own data.\n",
    "\n",
    "**Figure B:** This shows how we model the two participants with the state-based (sighted) model. In other words, we fit the sighted model to both blind and sighted data. One model, two sets of data. Interestingly, it seems the sighted model has a higher likelihood for explaining the blind data than the sighted data.\n",
    "\n",
    "However, if we use the fitted parameters found in figure B and *simulate* using the sighted model, we find that the behaviour produced (in the form of learning curves; **figure C**) looks like the sighted behaviour in figure A. But when simulating the sighted model with the parameters fit to *blind data* it produces behaviour that doesn't look like the behaviour in figure A.\n",
    "\n",
    "Got it? Cool. Let's describe the task we will use for this experiment.\n",
    "\n",
    "## The experimental task\n",
    "\n",
    "Imagine there are three different pictures that can be shown to you, and you have to learn which of three buttons to push depending on the picture. That's the \"sighted\" model. For the \"blind\" model, imagine that you are, well, blind. You get told when it's time to push a button again, and maybe you will learn the pattern, maybe you won't. Sounds like a horrifying task now that I put it that way.\n",
    "\n",
    "The paper describes the basic rules of the task, where we have three stimuli $s_1, s_2, s_3$ and three actions $a_1, a_2, a_3$.\n",
    "\n",
    "$a_1$ is the correct choice for $s_1, s_2$\n",
    "\n",
    "$a_3$ is the correct choice for $s_3$\n",
    "\n",
    "$a_2$ does nothing\n",
    "\n",
    "From the matlab code we see that we have 10 blocks, and learning values are reset at the start of each block. Every block has 45 trials.\n",
    "\n",
    "Since python has 0-index we will use that so in the following code we have three stimuli $s_0, s_1, s_2$ and three actions $a_0, a_1, a_2$.\n",
    "\n",
    "$a_0$ is the correct choice for $s_0, s_1$\n",
    "\n",
    "$a_2$ is the correct choice for $s_2$\n",
    "\n",
    "$a_1$ does nothing\n",
    "\n",
    "## Library and function imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "from numba import njit, int32, float64\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "from SimulationFunctions.choose import choose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Description of models\n",
    "\n",
    "Both models use a \"basic\" reinforcement learning algorithm like Model 3 that we have used in previous boxes. But in the previous task used - the two armed bandit - there was never a state, only actions. Well, I guess you could say there was an implicit single ever-lasting state. That's in essence what the \"blind\" model does; it has no concept of different states but tries to learn the values of each action anyway.\n",
    "\n",
    "In appendix 5 of the paper it's further described that we have two learning rates, $\\alpha_{pos}$ and $\\alpha_{neg}$, for positive reward prediction error and negative prediction error, respectively.\n",
    "\n",
    "$\\alpha_{neg}$ is set to 0 for all simulations and same for the likelihood functions in the Matlab code, so we will do the same here. In practice, we thus only update action values if the prediction error is positive, i.e. if the reward received is larger than predicted.\n",
    "\n",
    "Choices are again made by softmax, using $\\beta$ as our inverse temperature.\n",
    "\n",
    "## Simulation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "@njit\n",
    "def simulate_blind(stimuli, alpha_pos, beta):\n",
    "\n",
    "    alpha_neg = 0\n",
    "\n",
    "    states = np.zeros(10 * 45, dtype=int32)  # 10 blocks each with 45 trials\n",
    "    actions = np.zeros(10 * 45, dtype=int32)\n",
    "    rewards = np.zeros(10 * 45, dtype=int32)\n",
    "\n",
    "    # index represents state/stimuli\n",
    "    correct_actions = np.array([0, 0, 2], dtype=int32)\n",
    "\n",
    "    for block in range(10):\n",
    "\n",
    "        Q = np.ones(3) / 3  # each action has 1/3 change to start with\n",
    "\n",
    "        for trial in range(45):\n",
    "\n",
    "            observation = stimuli[trial]\n",
    "            probchoice = np.exp(beta * Q) / np.sum(np.exp(beta * Q))\n",
    "\n",
    "            action = choose(np.array([0, 1, 2], dtype=int32), probchoice)\n",
    "            reward = action == correct_actions[observation]\n",
    "\n",
    "            delta = reward - Q[action]\n",
    "            if delta > 0:\n",
    "                Q[action] += alpha_pos * delta\n",
    "            elif delta < 0:  # paper doesnt define delta == 0\n",
    "                Q[action] += alpha_neg * delta\n",
    "\n",
    "            save_index = block * 45 + trial\n",
    "            states[save_index] = observation\n",
    "            actions[save_index] = action\n",
    "            rewards[save_index] = reward\n",
    "\n",
    "    return states, actions, rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "@njit\n",
    "def simulate_sights(stimuli, alpha_pos, beta):\n",
    "\n",
    "    alpha_neg = 0\n",
    "\n",
    "    states = np.zeros(10 * 45, dtype=int32)\n",
    "    actions = np.zeros(10 * 45, dtype=int32)\n",
    "    rewards = np.zeros(10 * 45, dtype=int32)\n",
    "\n",
    "    correct_actions = np.array([0, 0, 2], dtype=int32)\n",
    "\n",
    "    for block in range(10):\n",
    "\n",
    "        Q = np.ones((3, 3)) / 3.0\n",
    "\n",
    "        for trial in range(45):\n",
    "\n",
    "            observation = stimuli[trial]\n",
    "            Q_row = Q[observation]\n",
    "            prob_choice = np.exp(beta * Q_row) / np.sum(np.exp(beta * Q_row))\n",
    "\n",
    "            action = choose(np.array([0, 1, 2], dtype=int32), prob_choice)\n",
    "            reward = action == correct_actions[observation]\n",
    "\n",
    "            delta = reward - Q[observation, action]\n",
    "            if delta > 0:\n",
    "                Q[observation, action] += alpha_pos * delta\n",
    "            elif delta < 0:\n",
    "                Q[observation, action] += alpha_neg * delta\n",
    "\n",
    "            save_index = block * 45 + trial\n",
    "            states[save_index] = observation\n",
    "            actions[save_index] = action\n",
    "            rewards[save_index] = reward\n",
    "\n",
    "    return states, actions, rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Likelihood function\n",
    "\n",
    "We won't fit the blind model so we only need a likelihood function for the sighted version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "\n",
    "@njit\n",
    "def llh_sights(parameters, states, actions, rewards):\n",
    "\n",
    "    alpha_pos = parameters[0]\n",
    "    beta = parameters[1]\n",
    "    alpha_neg = 0\n",
    "\n",
    "    trialcount = len(actions)\n",
    "    choice_probs = np.zeros(trialcount)\n",
    "\n",
    "    for trial in range(trialcount):\n",
    "\n",
    "        if trial % 45 == 0:\n",
    "            Q = np.ones((3, 3)) / 3.0\n",
    "\n",
    "        observation = states[trial]\n",
    "        Q_row = Q[observation]\n",
    "        prob_choice = np.exp(beta * Q_row) / np.sum(np.exp(beta * Q_row))\n",
    "\n",
    "        action = actions[trial]\n",
    "        choice_probs[trial] = prob_choice[action]\n",
    "\n",
    "        reward = rewards[trial]\n",
    "        delta = reward - Q[observation, action]\n",
    "        if delta > 0:\n",
    "            Q[observation, action] += alpha_pos * delta\n",
    "        elif delta < 0:\n",
    "            Q[observation, action] += alpha_neg * delta\n",
    "\n",
    "    loglikelihood = np.sum(np.log(choice_probs))\n",
    "    return -loglikelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The posterior function is used to get the likelihood values for figure B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def posterior(parms, states, actions, rewards):\n",
    "\n",
    "    alpha_pos = parms[0]\n",
    "    beta = parms[1]\n",
    "    alpha_neg = 0\n",
    "\n",
    "    trialcount = len(actions)\n",
    "    likehoods = np.zeros(trialcount)\n",
    "\n",
    "    for trial in range(trialcount):\n",
    "\n",
    "        if trial % 45 == 0:\n",
    "            Q = np.ones((3, 3)) / 3.0\n",
    "\n",
    "        obs = states[trial]\n",
    "        Q_row = Q[obs]\n",
    "        probs = np.exp(beta * Q_row) / np.sum(np.exp(beta * Q_row))\n",
    "        action = actions[trial]\n",
    "        reward = rewards[trial]\n",
    "        delta = reward - Q[obs, action]\n",
    "        if delta > 0:\n",
    "            Q[obs, action] += alpha_pos * delta\n",
    "        elif delta < 0:\n",
    "            Q[obs, action] += alpha_neg * delta\n",
    "\n",
    "        likehoods[trial] = probs[action]\n",
    "\n",
    "    return likehoods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "`fitRL` wraps our likelihood and posterior functions. We also run the minimization 10 times to find the best global optimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def fitRL(states, actions, rewards):\n",
    "\n",
    "    bounds = [(0.01, 0.99), (0.1, 20)]  # alpha, beta\n",
    "    best = 9999\n",
    "    best_parms = []\n",
    "\n",
    "    for _ in range(10):\n",
    "        guess = (np.random.rand(), np.random.rand() * 10)  # alpha, beta\n",
    "        result = minimize(\n",
    "            llh_sights, guess, args=(states, actions, rewards), bounds=bounds)\n",
    "        if result.fun < best:\n",
    "            best_parms = result.x\n",
    "\n",
    "    likes = posterior(best_parms, states, actions, rewards)\n",
    "\n",
    "    return best_parms, likes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "`summarize_per_stimuli` averages performance results across each stimuli. We have 45 trials in each block, and stimuli are presented in order, so we get $45 / 3 = 15$ trials on the x-axis for plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_per_stimuli(series):\n",
    "\n",
    "    stim1 = np.array([series[np.arange(block * 45, (block + 1) * 45, 3)] for block in range(10)])\n",
    "    stim2 = np.array([series[np.arange(block * 45 + 1, (block + 1) * 45, 3)] for block in range(10)])\n",
    "    stim3 = np.array([series[np.arange(block * 45 + 2, (block + 1) * 45, 3)] for block in range(10)])\n",
    "\n",
    "    return np.mean((stim1 + stim2 + stim3) / 3, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulate, fit and simulate again!\n",
    "\n",
    "Now for our main loop. We simulate the blind model, then fit the sighted model to this data and save the fitting result. That fitting result - the estimated values for alpha and beta - is then used to simulate the sighted model.\n",
    "\n",
    "Then we simulate the sighted model, fit the sighted model to that data and finally simulate the sighted model again using the estimated parameter values we just got.\n",
    "\n",
    "### Experimental parameters\n",
    "\n",
    "`sim_count` defines how many loops of sim-fitting we will do.\n",
    "\n",
    "Default simulation parameters are the specified values from the paper so we get similar performance curves in figure A for both models. Matlab code has slightly enlarged ranges for these values, still producing similar learning curves, that can be uncommented for some extra experimentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "sim_count = 100  # how many loops in total\n",
    "\n",
    "# np.tile is not supported by numba but is the same every loop anyway\n",
    "stimuli = np.tile(np.array([0, 1, 2], dtype=int), int(45 / 3))\n",
    "# fig a data\n",
    "sim_blinds = []\n",
    "sim_sights = []\n",
    "# fig b data\n",
    "like_blinds = []\n",
    "like_sights = []\n",
    "# fig c data\n",
    "simfit_blind = []\n",
    "simfit_sight = []\n",
    "\n",
    "for simulation in range(sim_count):\n",
    "\n",
    "    print(f\"simulation {simulation}\")\n",
    "\n",
    "    # blind model\n",
    "    # alpha_pos = 0.3 + 0.4 * np.random.rand()  # paper says this is set but okay\n",
    "    alpha_pos = 0.5\n",
    "    beta = 6.5\n",
    "    # beta = 4 + 5 * np.random.rand()\n",
    "    states, actions, rewards = simulate_blind(\n",
    "        stimuli, alpha_pos, beta)\n",
    "    sim_blinds.append(summarize_per_stimuli(rewards))\n",
    "\n",
    "    parms, likes = fitRL(states, actions, rewards)\n",
    "    like_blinds.append(summarize_per_stimuli(likes))\n",
    "\n",
    "    # simulation of _sighted_ model using blind data\n",
    "    states, actions, rewards = simulate_sights(\n",
    "        stimuli, parms[0], parms[1]\n",
    "    )\n",
    "    simfit_blind.append(summarize_per_stimuli(rewards))\n",
    "\n",
    "    # state-based/sighted model\n",
    "    # alpha_pos = 0.6 + 0.1 * np.random.rand()\n",
    "    alpha_pos = 0.65\n",
    "    beta = 2\n",
    "    states, actions, rewards = simulate_sights(\n",
    "        stimuli, alpha_pos, beta)\n",
    "    sim_sights.append(summarize_per_stimuli(rewards))\n",
    "\n",
    "    parms, likes = fitRL(states, actions, rewards)\n",
    "    like_sights.append(summarize_per_stimuli(likes))\n",
    "\n",
    "    states, actions, rewards = simulate_sights(\n",
    "        stimuli, parms[0], parms[1])\n",
    "    simfit_sight.append(summarize_per_stimuli(rewards))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "## Figure A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "sns.set(rc={\"figure.figsize\": (3, 3), \"figure.dpi\": 100})\n",
    "columns_a = ['sim', 'trial', 'model', 'p_correct']\n",
    "\n",
    "df_a = pd.DataFrame(columns=columns_a)\n",
    "for simnum in range(sim_count):\n",
    "    rows = [(simnum, x, 'blind', sim_blinds[simnum][x]) for x in range(15)]\n",
    "    temp_df = pd.DataFrame(columns=columns_a, data=rows)\n",
    "    df_a = pd.concat([df_a, temp_df])\n",
    "\n",
    "    rows = [(simnum, x, 'sights', sim_sights[simnum][x]) for x in range(15)]\n",
    "    temp_df = pd.DataFrame(columns=columns_a, data=rows)\n",
    "    df_a = pd.concat([df_a, temp_df])\n",
    "\n",
    "fig = sns.lineplot(data=df_a, x='trial', y='p_correct', hue='model')\n",
    "fig.set(ylim = (0.2, 1));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "## Figure B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "columns_b = ['simcount', 'trial', 'data_model', 'p_like']\n",
    "\n",
    "df_b = pd.DataFrame(columns=columns_b)\n",
    "for simnum in range(sim_count):\n",
    "    rows = [(simnum, x, 'blind', like_blinds[simnum][x]) for x in range(15)]\n",
    "    temp_df = pd.DataFrame(columns=columns_b, data=rows)\n",
    "    df_b = pd.concat([df_b, temp_df])\n",
    "\n",
    "    rows = [(simnum, x, 'sights', like_sights[simnum][x]) for x in range(15)]\n",
    "    temp_df = pd.DataFrame(columns=columns_b, data=rows)\n",
    "    df_b = pd.concat([df_b, temp_df])\n",
    "\n",
    "fig = sns.lineplot(data=df_b, x='trial', y='p_like', hue='data_model')\n",
    "fig.set(ylim = (0.2, 1));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "# Figure C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "columns_c = ['simcount', 'trial', 'sim_model', 'p_correct']\n",
    "\n",
    "df_c = pd.DataFrame(columns=columns_c)\n",
    "for simnum in range(sim_count):\n",
    "    rows = [(simnum, x, 'blind', simfit_blind[simnum][x]) for x in range(15)]\n",
    "    temp_df = pd.DataFrame(columns=columns_c, data=rows)\n",
    "    df_c = pd.concat([df_c, temp_df])\n",
    "\n",
    "    rows = [(simnum, x, 'sights', simfit_sight[simnum][x]) for x in range(15)]\n",
    "    temp_df = pd.DataFrame(columns=columns_c, data=rows)\n",
    "    df_c = pd.concat([df_c, temp_df])\n",
    "\n",
    "fig = sns.lineplot(data=df_c, x='trial', y='p_correct', hue='sim_model')\n",
    "fig.set(ylim = (0.2, 1));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Plotting all together\n",
    "\n",
    "nice to see plots next to each other, but to do this we need to have the same column names which may or may not be more confusing. especially for the middle plot, we can't get seaborn to change its y-axis name as it should be likelihood and not p(correct)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_a = df_a.rename(columns={'sim': 'simcount'})\n",
    "df_a['fig'] = 'a'\n",
    "\n",
    "df_b = df_b.rename(columns={'data_model': 'model'})\n",
    "df_b = df_b.rename(columns={'p_like': 'p_correct'})\n",
    "df_b['fig'] = 'b'\n",
    "\n",
    "df_c = df_c.rename(columns={'sim_model': 'model'})\n",
    "df_c['fig'] = 'c'\n",
    "\n",
    "df_combo = df_a.append(df_b).append(df_c)\n",
    "\n",
    "grid = sns.FacetGrid(df_combo, col='fig', hue='model')\n",
    "grid.map(sns.lineplot, 'trial', 'p_correct')\n",
    "grid.add_legend()\n",
    "grid.set_xlabels(\"time step\")\n",
    "grid.set(ylim=(0.2, 1), xlim=(-1, 15), xticks=[0, 5, 10, 14])\n",
    "grid.set_ylabels(\"p(correct)\")\n",
    "# grid.despine()\n",
    "axes = grid.axes.flatten()\n",
    "axes[0].set_title(\"'subject' learning curves\")\n",
    "axes[0].set_ylabel(\"p(correct)\")\n",
    "axes[1].set_title(\"likelihood of \\nstate-based RL model\")\n",
    "axes[1].set_ylabel(\"likelihood of choice\")\n",
    "axes[2].set_title(\"simulated learning curves \\nfrom state-based RL\")\n",
    "axes[2].set_ylabel(\"p(correct)\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
