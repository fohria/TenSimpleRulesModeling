{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "\n",
    "# Box3 - contending with multiple local maxima\n",
    "\n",
    "This box was confusing to me. The task and model used are different from the task and models in Box2 and Boxes 4-7. The text in the main paper says there are two parameters, learning rate $\\alpha$ and $\\rho$ which controls working memory influence. However, appendix 4 mentions four parameters and refers to a paywalled paper (Collins & Frank, 2012) for a complete description of the task used. When going through the Matlab code we see there are indeed four parameters, but one of these, $K$, is kept constant for simulations and recovery. So there are in fact three parameters we use for simulation and recovery here.\n",
    "\n",
    "I mention this because I *think* I've gotten the task and model right, but there may be some discrepancies left. However, this shouldn't matter for the principles shown.\n",
    "\n",
    "## Experimental task\n",
    "\n",
    "Our simulated participants will partake in a task where they see a picture (stimulus) and through trial-and-error have to learn which of three buttons (our actions or choices) to push to receive a reward. The experiment has multiple blocks, each with a different number of stimuli ranging from 2 to 6 (`setsize` variable below). Every block is repeated three times, so we have a total of $5 * 3 = 15$ blocks. Within a block, each stimuli is repeated 15 times giving us a total of $(2 + 3 + 4 + 5 + 6) * 15 = 300$ trials, and including the block repeats we thus have $300 * 3 = 900$ trials in total.\n",
    "\n",
    "## Learning model\n",
    "\n",
    "This model uses a mix of reinforcement learning (RL) and working memory (WM). So we have the standard RL with values for each combination of stimuli (state) and action that's updated every trial like:\n",
    "\n",
    "$$Q(state, action) = Q(state, action) + \\alpha * (reward - Q(state, action))$$\n",
    "\n",
    "and then we have similar state, action values for WM that are just set to the reward received:\n",
    "\n",
    "$$WM(state, choice) = reward$$\n",
    "\n",
    "Choices are made using softmax with inverse temperature $\\beta$ for RL:\n",
    "\n",
    "$$p(action) = \\frac{e^{\\beta * Q(state)}}{\\sum{e^{\\beta * Q(state)}}}$$\n",
    "\n",
    "and for WM the same softmax is used, but using a constant of $50$ instead of $\\beta$ which essentially creates a fully \"greedy\" behaviour, almost always picking the highest valued action.\n",
    "\n",
    "The final choice probabilities are calculated with a mixture policy:\n",
    "\n",
    "$$p(action) = (1 - w) * p_{RL} + w * p_{WM}$$\n",
    "\n",
    "where $w = \\rho * min(1, \\frac{K}{ns})$, where $ns$ is number of stimuli and $K$ can be seen as scaling the mixture weight in proportion to working memory capacity vs number of stimuli.\n",
    "\n",
    "## what we will do\n",
    "\n",
    "we will first simulate one participant, then use that data to plot a likelihood heatmap for many combinations of the four parameters.\n",
    "\n",
    "## import libraries and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# autoreload files in ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from numba import njit, int32\n",
    "from scipy.optimize import minimize\n",
    "from itertools import product\n",
    "\n",
    "from SimulationFunctions.simulate_RLWM import simulate_RLWM_block\n",
    "from LikelihoodFunctions.lik_RLWM import likelihood_RLWM_block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## experimental parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# simulation parameters\n",
    "real_alpha = 0.1\n",
    "real_beta = 10\n",
    "real_rho = 0.9\n",
    "real_K = 4\n",
    "\n",
    "# range of parameter values to calculate likelihoods for\n",
    "# alphas = np.arange(0.05, 1, 0.2)\n",
    "# betas = np.arange(1, 30, 10)\n",
    "# rhos = np.arange(0.05, 1, 0.2)\n",
    "# Ks = np.arange(2, 7)\n",
    "\n",
    "# finer grid\n",
    "alphas = np.arange(0.05, 0.51, 0.01)\n",
    "betas = np.append([1], np.arange(4, 20, 2))\n",
    "rhos = np.arange(0.5, 1.0, 0.01)\n",
    "# Ks = np.arange(2, 7)\n",
    "Ks = np.array([4])  # matlab code keeps this constant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## simulation\n",
    "\n",
    "the imported simulation function simulates one block of the task, so we construct the blocks here for clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_participant():\n",
    "    data = []\n",
    "    block = -1  # to match 0-indexing and keep track of block\n",
    "\n",
    "    for _ in range(3):  # 3 repetitions of each setsize\n",
    "        for setsize in range(2, 7):  # up to but not including 7\n",
    "\n",
    "            block += 1\n",
    "\n",
    "            stimuli, choices, rewards = simulate_RLWM_block(\n",
    "                real_alpha, real_beta, real_rho, real_K, setsize)\n",
    "\n",
    "            # rows = [\n",
    "            #     (stimulus, choice, reward, setsize, block, trial)\n",
    "            #     for trial, (stimulus, choice, reward)\n",
    "            #     in enumerate(zip(stimuli, choices, rewards))\n",
    "            # ]\n",
    "            #\n",
    "            # for row in rows:\n",
    "            #     data.append(row)\n",
    "\n",
    "            data.append([stimuli, choices, rewards])  #single line\n",
    "\n",
    "            # data.append(rows)\n",
    "\n",
    "    return data\n",
    "\n",
    "data = simulate_participant()\n",
    "\n",
    "# column_names = [\n",
    "#     'stimulus', 'choice', 'reward', 'setsize', 'block', 'trial'\n",
    "# ]\n",
    "# df = pd.DataFrame(columns=column_names, data=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The matlab code has a manual quirk here to check if the performance decreases with increasing setsize. I suspect parameter recovery is better when this pattern is true, but unfortunately there is no discussion in the code or the paper about this. I've chosen not to care about this, but will comment on the problem of recovery after our first heatmap plot.\n",
    "\n",
    "Actually, let's check that and see how/if it affects the beta recovery later. Later: actually, yes, with simulation results like this:\n",
    "\n",
    "setsize 2 mean: 0.9333333333333335\n",
    "setsize 3 mean: 0.8962962962962963\n",
    "setsize 4 mean: 0.8944444444444444\n",
    "setsize 5 mean: 0.8755555555555556\n",
    "setsize 6 mean: 0.8481481481481481\n",
    "\n",
    "We get fitted parameters that are very very close to real parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_rewards = {2: 0, 3: 0, 4: 0, 5: 0, 6: 0}\n",
    "for block in data:\n",
    "    setsize = len(np.unique(block[0]))\n",
    "    mean_rewards[setsize] += np.mean(block[2])\n",
    "    # print(setsize)\n",
    "\n",
    "for setsize in mean_rewards:\n",
    "    print(f\"setsize {setsize} mean: {mean_rewards[setsize] / 3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## calculate likelihood for all parameter combinations\n",
    "\n",
    "we will now go through all the combinations of alpha, beta, rho and K we have and for each combination calculate the summed loglikelihood of all actions taken.\n",
    "\n",
    "what this means is that we go through the choices from the simulation, and for each trial calculate the probability of that choice being made, based on a specific set of parameter values. So for parameter values very far from the true parameter values we used to run the simulation, we should get a much lower \"score\" (likelihood) than for parameter values very close to the true values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loglikes = []\n",
    "\n",
    "for alpha, beta, rho, K in product(alphas, betas, rhos, Ks):\n",
    "\n",
    "    loglike_allblocks = 0\n",
    "    for block in range(15):\n",
    "\n",
    "        # blockdata = np.array(df.query('block == @block'))\n",
    "        blockdata = np.array(data[block])  # numba wants numpy array\n",
    "\n",
    "        loglike_allblocks += likelihood_RLWM_block(\n",
    "            alpha, beta, rho, K, blockdata)\n",
    "\n",
    "    loglikes.append(\n",
    "        (alpha, beta, rho, K, loglike_allblocks))\n",
    "\n",
    "\n",
    "brute_results = pd.DataFrame(\n",
    "    columns = ['alpha', 'beta', 'rho', 'K', 'loglike'],\n",
    "    data = loglikes\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "I'm allergic to importing matplotlib just to do seemingly simple things such as modifying the labels on the x/y axis. Seaborn works okay for most things without such sillyness, but heatmap is a bit too complicated so it's cumbersome to customize. So we have to manually round the parameter values (because python often generates 7.000000000000001 instead of 7.0), and then tell `sns.heatmap` we only want every fifth tick label.\n",
    "\n",
    "If you happen to know how I can give heatmap a list of y labels like `[0.5, 0.6, 0.7, 0.8, 0.9]` and it will use that automagically, please let me know!\n",
    "\n",
    "A nice and convenient thing about seaborn though, is that when you run several plot commands in succession they'll automatically overlay on the same figure, and we exploit that to mark the 'real' parameters used for the simulation (the red x marks the spot!) and the best parameters found (the black dot).\n",
    "\n",
    "## create pivot table and plot likelihood surface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "brute_results.alpha = brute_results.alpha.apply(lambda x: round(x, 2))\n",
    "brute_results.rho = brute_results.rho.apply(lambda x: round(x, 2))\n",
    "\n",
    "hot_data = brute_results[[\n",
    "    'alpha', 'rho', 'loglike'\n",
    "]].pivot_table(\n",
    "    values = 'loglike',\n",
    "    index = 'rho',\n",
    "    columns = 'alpha',\n",
    "    aggfunc = 'mean'\n",
    ")\n",
    "\n",
    "# setup the look of our plot\n",
    "sns.set(rc={\n",
    "    \"figure.figsize\": (8, 6),\n",
    "    \"figure.dpi\": 100,\n",
    "    \"lines.markersize\": 10,\n",
    "    \"font.size\": 10\n",
    "})\n",
    "\n",
    "# heatmap coordinates are based on column indeces\n",
    "x_min = hot_data.columns.min()\n",
    "x_max = hot_data.columns.max()\n",
    "x_scale = (len(hot_data.columns) - 1) / (x_max - x_min)\n",
    "y_min = hot_data.index.min()\n",
    "y_max = hot_data.index.max()\n",
    "y_scale = (len(hot_data.index) - 1) / (y_max - y_min)\n",
    "\n",
    "x_mark_real = round((real_alpha - x_min) * x_scale)\n",
    "y_mark_real = round((real_rho - y_min) * y_scale)\n",
    "\n",
    "# get the best result and its coordinates for the heatmap\n",
    "best_index = brute_results.loglike.idxmax()\n",
    "best_result = brute_results.iloc[best_index, :]\n",
    "best_alpha = best_result['alpha']\n",
    "best_rho = best_result['rho']\n",
    "\n",
    "x_mark_best = round((best_alpha - x_min) * x_scale)\n",
    "y_mark_best = round((best_rho - y_min) * y_scale)\n",
    "\n",
    "# plot the heatmap, only print every 5 x/y ticks\n",
    "fig = sns.heatmap(\n",
    "    hot_data, xticklabels = 5, yticklabels = 5, cmap = 'viridis')\n",
    "\n",
    "fig = sns.scatterplot(\n",
    "    x = [x_mark_real + 0.5],  # put mark in middle of the heatmap box\n",
    "    y = [y_mark_real + 0.5],\n",
    "    marker=\"X\",\n",
    "    color=\"red\"\n",
    ")\n",
    "fig = sns.scatterplot(\n",
    "    x = [x_mark_best + 0.5],\n",
    "    y = [y_mark_best + 0.5],\n",
    "    marker=\".\",\n",
    "    color=\"black\"\n",
    ")\n",
    "print(f\"real alpha, rho: {real_alpha}, {real_rho}\")\n",
    "print(f\"best alpha, rho: {best_alpha}, {best_rho}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Note that seaborn/matplotlib puts the upmost part of the y-axis labels where their corresponding boxes are, so it might look a bit off vertically.\n",
    "\n",
    "Now, in the case of our brute force search above, we found that the best likelihood wasn't actually found at the real/simulated parameter values. If we re-run the simulation and brute force again we may or may not get closer.\n",
    "\n",
    "This is unfortunately something we have to accept; the problem of recovering parameter values is a statistical one and therefore we are going to get slightly different results every time. We will see in the next box, Box4, how to check the overall performance of parameter recovery."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## using optimization algorithms to find parameters\n",
    "\n",
    "Instead of using the brute force search above, SciPy has a nice optimization package with a function called `minimize`. We can feed our likelihood function into `minimize` and have it find the best fit for us.\n",
    "\n",
    "The downside of this function is that it can get stuck in local minima, so it's a good idea to give it a bunch of random starting points. The brute force method doesn't have this problem as it will methodically go through the entire parameter space with the \"resolution\" (number of parameter combinations) we have chosen.\n",
    "\n",
    "The upside is it can often be much quicker, especially if we create an optimized likelihood function with numba.\n",
    "\n",
    "## optimized likelihood function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "First we need to reshape our simulation data, because numpy/numba doesn't like \"ragged\" arrays, meaning arrays of different lengths.\n",
    "\n",
    "Somewhat ugly but can be cleaned if we save the simulation data smarter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def reshape_simdata(data):\n",
    "    trials = 0\n",
    "    stimuli = []\n",
    "    choices = []\n",
    "    rewards = []\n",
    "    block_indeces = []\n",
    "\n",
    "    for block in data:\n",
    "        # stimuli, choices, rewards\n",
    "        for trial in range(len(block[0])):\n",
    "            stimuli.append(block[0][trial])\n",
    "            choices.append(block[1][trial])\n",
    "            rewards.append(block[2][trial])\n",
    "            trials += 1\n",
    "        block_indeces.append(trials)\n",
    "\n",
    "    stimuli = np.array(stimuli)\n",
    "    choices = np.array(choices)\n",
    "    rewards = np.array(rewards)\n",
    "    block_indeces = np.array(block_indeces)\n",
    "\n",
    "    return stimuli, choices, rewards, block_indeces\n",
    "\n",
    "stimuli, choices, rewards, block_indeces = reshape_simdata(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "scipy's `minimize` function wants all the parameters in one variable. It also is, as you may have inferred from the name, a minimization function so we need to return the negative loglikelihood.\n",
    "\n",
    "We follow the matlab code here in keeping K as a constant, but it can of course be added to the parameters we ask minimize to find."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit\n",
    "def recover_participant(\n",
    "    parameters, K, stimuli, choices, rewards, block_indeces):\n",
    "# def recover_participant(\n",
    "#     parameters, stimuli, choices, rewards, block_indeces):\n",
    "\n",
    "    alpha = parameters[0]\n",
    "    beta = parameters[1]\n",
    "    rho = parameters[2]\n",
    "    # K = parameters[3]\n",
    "\n",
    "    loglike_allblocks = 0\n",
    "\n",
    "    low_index = 0\n",
    "    for index in block_indeces:\n",
    "\n",
    "        high_index = index\n",
    "        blocksize = high_index - low_index\n",
    "        # numba wants an empty array to fill\n",
    "        blockdata = np.empty((3, blocksize), dtype=int32)\n",
    "        blockdata[0] = stimuli[low_index:high_index]\n",
    "        blockdata[1] = choices[low_index:high_index]\n",
    "        blockdata[2] = rewards[low_index:high_index]\n",
    "\n",
    "        loglike_allblocks += likelihood_RLWM_block(\n",
    "            alpha, beta, rho, K, blockdata)\n",
    "        low_index = index\n",
    "\n",
    "    return -loglike_allblocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now we run minimize 10 times and save the results to then extract the best result. It's almost always a good practice to provide bounds to the method, otherwise we may get over/underflow errors. This can happen anyway, but luckily `minimize` tells us if the operation was a success or not which is why we have that additional check before appending the result.\n",
    "\n",
    "In the below call to minimize, `start_guess` will automagically become the `parameters` in the function `recover_participant` we defined above, while the `args` become the rest of the arguments to the function, in order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "def fit_participant(stimuli, choices, rewards, block_indeces):\n",
    "    # alpha, beta, rho\n",
    "    bounds = [(0.05, 0.99), (1, 30), (0.01, 0.99)]\n",
    "    # bounds = [(0.01, 0.99), (1, 30), (0.01, 0.99), (2, 6)]\n",
    "    results = []\n",
    "    for _ in range(10):\n",
    "        start_guess = [\n",
    "            np.random.rand(),       # alpha\n",
    "            np.random.uniform(20),  # beta\n",
    "            np.random.rand(),       # rho\n",
    "            # real_K                  # K\n",
    "        ]\n",
    "        result = minimize(\n",
    "            recover_participant,\n",
    "            start_guess,\n",
    "            args = (real_K, stimuli, choices, rewards, block_indeces),\n",
    "            # args = (stimuli, choices, rewards, block_indeces),\n",
    "            bounds = bounds\n",
    "        )\n",
    "        if result.success is True:\n",
    "            results.append(result)\n",
    "\n",
    "    return results\n",
    "\n",
    "results = fit_participant(stimuli, choices, rewards, block_indeces)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now we can get the best result from minimize and plot on our heatmap for comparison, and complete figure A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get best minimize result and coordinates for heatmap\n",
    "best_minimize_index = np.argmin(np.array([res.fun for res in results]))\n",
    "best_minimize = results[best_minimize_index]\n",
    "# minimize result's x variable holds [alpha, beta, rho]\n",
    "best_fit_alpha = best_minimize.x[0]\n",
    "best_fit_rho = best_minimize.x[2]\n",
    "\n",
    "x_mark_fit = round((best_fit_alpha - x_min) * x_scale)\n",
    "y_mark_fit = round((best_fit_rho - y_min) * y_scale)\n",
    "\n",
    "# cmap = sns.color_palette(\"colorblind\", as_cmap=True)\n",
    "cmap = 'viridis'\n",
    "# cmap = 'Spectral'\n",
    "# plot the heatmap, only print every 5 x/y ticks\n",
    "fig = sns.heatmap(hot_data, xticklabels=5, yticklabels=5, cmap=cmap)\n",
    "\n",
    "fig = sns.scatterplot(\n",
    "    x = [x_mark_real + 0.5],\n",
    "    y = [y_mark_real + 0.5],\n",
    "    marker=\"X\",\n",
    "    color=\"red\"\n",
    ")\n",
    "fig = sns.scatterplot(\n",
    "    x = [x_mark_fit + 0.5],\n",
    "    y = [y_mark_fit + 0.5],\n",
    "    marker=\"*\",\n",
    "    color=\"blue\"\n",
    ")\n",
    "fig = sns.scatterplot(\n",
    "    x = [x_mark_best + 0.5],\n",
    "    y = [y_mark_best + 0.5],\n",
    "    marker=\".\",\n",
    "    color=\"black\"\n",
    ")\n",
    "\n",
    "print(f\"red x - real alpha, rho: {real_alpha}, {real_rho}\")\n",
    "print(f\"black dot - best brute force alpha, rho: {best_alpha}, {best_rho}\")\n",
    "print(f\"blue star - best minfitted alpha, rho: {best_fit_alpha}, {best_fit_rho}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "With a few iterations of `minimize` and enough parameter combinations for the brute force method, they mostly reach the same result. The brute force method is needed to plot the likelihood surface/heatmap which is good for checking your likelihood function works as expected. When you've done that it may be more convenient to use the `minimize` directly. An additional benefit of `minimize` is that it is usually able to find\n",
    "\n",
    "Speaking of checking likelihood function, in some situations our plots above give us results that look off. Meaning they are not at the \"max\" of the surface. That's not an error, because we are only plotting two out of three estimated parameters.\n",
    "\n",
    "So if we check what the likelihoods are in our hot data for the best results and the real values we see the real values indeed has a better likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"likelihood for best result: {hot_data.iloc[y_mark_best, x_mark_best]}\")\n",
    "print(f\"likelihood for real values: {hot_data.iloc[y_mark_real, x_mark_real]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "But if we go back to our dataframe with estimated values for the $\\beta$ parameter we can see that the best `loglike` is where it was plotted on the heatmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_loglike = brute_results.loglike.max()\n",
    "print(f\"best loglike: {best_loglike}\")\n",
    "print(brute_results.query(\"loglike == @best_loglike\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Beta estimation isn't great, but as previously mentioned, this is not something the paper discusses. Running the matlab code gives wildly different distances to the true beta depending on the simulation, so for the next step and figure, be aware that the plot doesn't show how far we are to the real parameters, only how far away we are from the best parameters we can fit.\n",
    "\n",
    "## how many starting guesses before we find global max?\n",
    "\n",
    "To create the lineplot in Box3 we run a simulation 10 times, each time using `minimize` to fit 10 times. We save all the results to a dataframe so we can compare the final parameter values found to previous ones and what iteration they were found at."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import euclidean\n",
    "\n",
    "result_rows = []\n",
    "for simfit in range(10):\n",
    "\n",
    "    simdata = simulate_participant()\n",
    "\n",
    "    stimuli, choices, rewards, block_indeces = reshape_simdata(simdata)\n",
    "\n",
    "    fit_results = fit_participant(\n",
    "        stimuli, choices, rewards, block_indeces)\n",
    "\n",
    "    fit_likelihoods = np.array([result.fun for result in fit_results])\n",
    "    best_fitindex = np.argmin(fit_likelihoods)\n",
    "    best_parameters = fit_results[best_fitindex].x\n",
    "\n",
    "    best_distance_so_far = 999\n",
    "    for iteration, result in enumerate(fit_results):\n",
    "        distance_to_best = euclidean(best_parameters, result.x)\n",
    "        if distance_to_best < best_distance_so_far:\n",
    "            best_distance_so_far = distance_to_best\n",
    "        result_rows.append(\n",
    "            (simfit,\n",
    "            iteration,\n",
    "            distance_to_best,\n",
    "            best_distance_so_far)\n",
    "        )\n",
    "\n",
    "\n",
    "columns = [\n",
    "    'sim iteration',\n",
    "    'fit iteration',\n",
    "    'dist to best',\n",
    "    'best dist so far'\n",
    "]\n",
    "result_data = pd.DataFrame(columns = columns, data = result_rows)\n",
    "\n",
    "sns.lineplot(\n",
    "    data = result_data,\n",
    "    x = 'fit iteration',\n",
    "    y = 'best dist so far'\n",
    ")\n",
    "\n",
    "# and log scale, removing the last iteration since it's 0\n",
    "fig = sns.lineplot(\n",
    "    data=result_data.query(\"`fit iteration` != 9\"),\n",
    "    x='fit iteration',\n",
    "    y='best dist so far'\n",
    ")\n",
    "fig.set(yscale=\"log\");"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
